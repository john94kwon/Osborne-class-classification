{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93984b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important!: This codebase was initially written for internal use and requires manual path adjustments and environment setup.\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "#Base code retrieved from J Cheminform 13, 7 (2021). https://doi.org/10.1186/s13321-021-00488-1\n",
    "\n",
    "\n",
    "# Seed\n",
    "SEED = 2333\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    \n",
    "# Model parameters\n",
    "NUMBER_EPOCHS = 9\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1E-4\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# GCN parameters\n",
    "GCN_FEATURE_DIM = 30\n",
    "GCN_HIDDEN_DIM = 128\n",
    "GCN_OUTPUT_DIM = 32   \n",
    "# Attention parameters\n",
    "DENSE_DIM = 16\n",
    "ATTENTION_HEADS = 4\n",
    "\n",
    "\n",
    "def load_features(label_number):\n",
    "    feature_matrix = np.load(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\masking\\\\GCN_node_norm_padded_GG\\\\\" + str(label_number) + '.npy').astype(np.float32)\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "def load_graph(label_number): \n",
    "    edge_matrix = np.load(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\masking\\\\GCN_contact_maps_norm_padded_GG\\\\\" + str(label_number) + '.npy').astype(np.float32)\n",
    "    return edge_matrix\n",
    "\n",
    "\n",
    "class ProDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.label = dataframe['index'].values\n",
    "        self.solubility = dataframe['Class'].values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_label = self.label[index]\n",
    "        solubility = self.solubility[index]\n",
    "        # L * 30\n",
    "        sequence_feature = load_features(sequence_label)\n",
    "        # L * L\n",
    "        sequence_graph = load_graph(sequence_label)\n",
    "        return sequence_label, solubility, sequence_feature, sequence_graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.solubility)\n",
    "    \n",
    "    \n",
    "    \n",
    "class GraphConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = input @ self.weight    # X * W\n",
    "        output = adj @ support           # A * X * W\n",
    "        if self.bias is not None:        # A * X * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(GCN_FEATURE_DIM, GCN_HIDDEN_DIM)\n",
    "        self.ln1 = nn.LayerNorm(GCN_HIDDEN_DIM)\n",
    "        self.gc2 = GraphConvolution(GCN_HIDDEN_DIM, GCN_OUTPUT_DIM)\n",
    "        self.ln2 = nn.LayerNorm(GCN_OUTPUT_DIM)\n",
    "        self.relu1 = nn.LeakyReLU(0.1,inplace=True)\n",
    "        self.relu2 = nn.LeakyReLU(0.1,inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t# x.shape = (seq_len, GCN_FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = self.gc1(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu1(self.ln1(x))\n",
    "        x = self.gc2(x, adj)\n",
    "        output = self.relu2(self.ln2(x))\t# output.shape = (seq_len, GCN_OUTPUT_DIM)\n",
    "        return output\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, dense_dim, n_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.dense_dim)\n",
    "        self.fc2 = nn.Linear(self.dense_dim, self.n_heads)\n",
    "\n",
    "    def softmax(self, input, axis=1):\n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size) - 1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = torch.softmax(input_2d, dim=1)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size) - 1)\n",
    "\n",
    "    def forward(self, input):  \t\t\t\t# input.shape = (1, seq_len, input_dim)\n",
    "        x = torch.tanh(self.fc1(input))  \t# x.shape = (1, seq_len, dense_dim)\n",
    "        x = self.fc2(x)  \t\t\t\t\t# x.shape = (1, seq_len, attention_hops)\n",
    "        x = self.softmax(x, 1)\n",
    "        attention = x.transpose(1, 2)  \t\t# attention.shape = (1, attention_hops, seq_len)\n",
    "        return attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.gcn = GCN()  # Assuming GCN() is defined elsewhere in your code\n",
    "        self.attention = Attention(GCN_OUTPUT_DIM, DENSE_DIM, ATTENTION_HEADS)  # Assuming Attention is defined\n",
    "        self.fc_final = nn.Linear(GCN_OUTPUT_DIM, 1)  # Only 1 output for binary classification\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = x.float()\n",
    "        x = self.gcn(x, adj)\n",
    "        x = x.unsqueeze(0)\n",
    "        att = self.attention(x)\n",
    "        node_feature_embedding = att @ x\n",
    "        node_feature_embedding_avg = torch.mean(node_feature_embedding, 1)\n",
    "        output = self.fc_final(node_feature_embedding_avg)\n",
    "        return output\n",
    "        # No need to squeeze as we're now handling binary output directly\n",
    "\n",
    "model = Model()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, data_loader, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the correct device\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Set up BCEWithLogitsLoss for binary classification\n",
    "    epoch_loss_train = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        model.optimizer.zero_grad()\n",
    "        _, solubility, sequence_features, sequence_graphs = data\n",
    "\n",
    "        # Move data to device\n",
    "        sequence_features = sequence_features.to(device).squeeze()\n",
    "        sequence_graphs = sequence_graphs.to(device).squeeze()\n",
    "        y_true = solubility.to(device).float()  # Ensure y_true is float for BCEWithLogitsLoss\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(sequence_features, sequence_graphs)  # Expecting y_pred shape: [batch_size, 1]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred.view(-1), y_true)   # Remove extra dimension if necessary\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_loss_train += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss_train_avg = epoch_loss_train / n_batches\n",
    "    return epoch_loss_train_avg\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    valid_pred = []\n",
    "    valid_true = []\n",
    "    valid_label = []\n",
    "\n",
    "    criterion = model.criterion  # Use the criterion defined in the model\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            sequence_label, Class, sequence_features, sequence_graphs = data\n",
    "            sequence_features = sequence_features.to(device).squeeze()\n",
    "            sequence_graphs = sequence_graphs.to(device).squeeze()\n",
    "            y_true = Class.to(device).float()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(sequence_features, sequence_graphs)\n",
    "            loss = criterion(y_pred.squeeze(0), y_true)\n",
    "\n",
    "            # Apply sigmoid to logits to get probabilities\n",
    "            y_pred_prob = torch.sigmoid(y_pred).cpu().numpy().tolist()\n",
    "            y_true = y_true.cpu().numpy().tolist()\n",
    "\n",
    "            valid_pred.extend(y_pred_prob)\n",
    "            valid_true.extend(y_true)\n",
    "            valid_label.extend(sequence_label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    epoch_loss_avg = epoch_loss / n_batches\n",
    "\n",
    "    return epoch_loss_avg, valid_true, valid_pred, valid_label\n",
    "\n",
    "def train(model, train_dataframe, valid_dataframe, fold=0):\n",
    "    train_loader = DataLoader(dataset=ProDataset(train_dataframe), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(dataset=ProDataset(valid_dataframe), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    train_losses = []\n",
    "    train_pearson = []\n",
    "    train_r2 = []\n",
    "    train_acc = []\n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "    train_auc = []\n",
    "    train_mcc = []\n",
    "    train_sensitivity = []\n",
    "    train_specificity = []\n",
    "\n",
    "    valid_losses = []\n",
    "    valid_pearson = []\n",
    "    valid_r2 = []\n",
    "    valid_acc = []\n",
    "    valid_precision = []\n",
    "    valid_recall = []\n",
    "    valid_f1 = []\n",
    "    valid_auc = []\n",
    "    valid_mcc = []\n",
    "    valid_sensitivity = []\n",
    "    valid_specificity = []\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(NUMBER_EPOCHS):\n",
    "        print(\"\\n========== Train epoch \" + str(epoch + 1) + \" ==========\")\n",
    "        model.train()\n",
    "        \n",
    "        print(train_loader)\n",
    "\n",
    "        epoch_loss_train_avg = train_one_epoch(model, train_loader, epoch + 1)\n",
    "        print(\"========== Evaluate Train set ==========\")\n",
    "        _, train_true, train_pred, _ = evaluate(model, train_loader)\n",
    "        result_train = analysis(train_true, train_pred)\n",
    "        \n",
    "        print(\"Train loss: \", np.sqrt(epoch_loss_train_avg))\n",
    "        print(\"Train acc: \", result_train['accuracy'])\n",
    "        print(\"Train precision: \", result_train['precision'])\n",
    "        print(\"Train recall: \", result_train['recall'])\n",
    "        print(\"Train F1: \", result_train['f1'])\n",
    "\n",
    "        train_losses.append(np.sqrt(epoch_loss_train_avg))\n",
    "        train_acc.append(result_train['accuracy'])\n",
    "        train_precision.append(result_train['precision'])\n",
    "        train_recall.append(result_train['recall'])\n",
    "        train_f1.append(result_train['f1'])\n",
    "\n",
    "\n",
    "        print(\"========== Evaluate Valid set ==========\")\n",
    "        epoch_loss_valid_avg, valid_true, valid_pred, valid_label = evaluate(model, valid_loader)\n",
    "        result_valid = analysis(valid_true, valid_pred)\n",
    "        \n",
    "        print(\"Valid loss: \", np.sqrt(epoch_loss_valid_avg))\n",
    "        print(\"Valid acc: \", result_valid['accuracy'])\n",
    "        print(\"Valid precision: \", result_valid['precision'])\n",
    "        print(\"Valid recall: \", result_valid['recall'])\n",
    "        print(\"Valid f1: \", result_valid['f1'])\n",
    "        \n",
    "        valid_losses.append(np.sqrt(epoch_loss_valid_avg))\n",
    "        valid_acc.append(result_valid['accuracy'])\n",
    "        valid_precision.append(result_valid['precision'])\n",
    "        valid_recall.append(result_valid['recall'])\n",
    "        valid_f1.append(result_valid['f1'])\n",
    "\n",
    "\n",
    "        print(epoch)\n",
    "        if epoch==(NUMBER_EPOCHS-1):\n",
    "            valid_final_dataframe = pd.DataFrame({'class_real': valid_true, 'class_predicted': valid_pred})\n",
    "            valid_final_dataframe.to_csv(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\binary\\\\GCN_glo_glu_CV\\\\\" + 'Fold' + str(fold) + \"EPOCH\"+ str(NUMBER_EPOCHS)+\"_valid_detail.csv\", header=True, sep=',')\n",
    "            print(\"saved!\")\n",
    "\n",
    "\n",
    "    # save calculation information\n",
    "    result_all = {\n",
    "        'Train_loss': train_losses,\n",
    "        'Train_binary_acc': train_acc,\n",
    "        'Train_precision': train_precision,\n",
    "        'Train_recall': train_recall,\n",
    "        'Train_f1': train_f1,\n",
    "        'Valid_loss': valid_losses,\n",
    "        'Valid_binary_acc': valid_acc,\n",
    "        'Valid_precision': valid_precision,\n",
    "        'Valid_recall': valid_recall,\n",
    "        'Valid_f1': valid_f1,\n",
    "        'Best_epoch': [best_epoch for _ in range(len(train_losses))]\n",
    "    }\n",
    "    \n",
    "    result = pd.DataFrame(result_all)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Fold\", str(fold), \"Best epoch at\", str(best_epoch))\n",
    "    result.to_csv(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\binary\\\\GCN_glo_glu_CV\\\\\" + \"Fold\" + str(fold) + \"_result.csv\", sep=',')\n",
    "    ##############################4################################\n",
    "            \n",
    "def train(model, train_dataframe):\n",
    "    train_loader = DataLoader(dataset=ProDataset(train_dataframe), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    train_losses = []\n",
    "    train_r2 = []\n",
    "    train_acc = []\n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(NUMBER_EPOCHS+1):\n",
    "        print(\"\\n========== Train epoch \" + str(epoch + 1) + \" ==========\")\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss_train_avg = train_one_epoch(model, train_loader, epoch + 1)\n",
    "        print(\"========== Evaluate Train set ==========\")\n",
    "        _, train_true, train_pred, train_label = evaluate(model, train_loader)\n",
    "        result_train = analysis(train_true, train_pred)\n",
    "        print(\"Train loss: \", np.sqrt(epoch_loss_train_avg))\n",
    "        print(\"Train binary acc: \", result_train['accuracy'])\n",
    "        print(\"Train precision: \", result_train['precision'])\n",
    "        print(\"Train recall: \", result_train['recall'])\n",
    "        print(\"Train F1: \", result_train['f1'])\n",
    "\n",
    "\n",
    "        train_losses.append(np.sqrt(epoch_loss_train_avg))\n",
    "        train_acc.append(result_train['accuracy'])\n",
    "        train_precision.append(result_train['precision'])\n",
    "        train_recall.append(result_train['recall'])\n",
    "        train_f1.append(result_train['f1'])\n",
    "        print(epoch)\n",
    "        print(NUMBER_EPOCHS)\n",
    "\n",
    "        if epoch==NUMBER_EPOCHS:\n",
    "            torch.save(model.state_dict(), os.path.join(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\binary\\\\GCN_glo_glu_CV\\\\train_all.pkl\")) ## \n",
    "            ##############################2############################\n",
    "   \n",
    "\n",
    "def analysis(y_true, y_pred):\n",
    "    # Flatten y_pred if it contains nested lists\n",
    "    y_pred_flat = [p[0] if isinstance(p, list) else p for p in y_pred]\n",
    "\n",
    "    # Convert predictions to class labels based on a threshold of 0.5\n",
    "    y_pred_labels = [1 if prob > 0.5 else 0 for prob in y_pred_flat]\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "    precision = precision_score(y_true, y_pred_labels, average='binary')\n",
    "    recall = recall_score(y_true, y_pred_labels, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred_labels, average='binary')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred_labels)\n",
    "\n",
    "    result = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def train_all(all_dataframe):\n",
    "    print(\"split_seed: \", SEED)\n",
    "    sequence_label = all_dataframe['index'].values\n",
    "    Class = all_dataframe['Class'].values\n",
    "    model=Model()\n",
    "    print(sequence_label)\n",
    "    \n",
    "    train(model, all_dataframe)\n",
    "    \n",
    "    \n",
    "def cross_validation(all_dataframe,fold_number=5):\n",
    "    print(\"split_seed: \", SEED)\n",
    "    sequence_label = all_dataframe['index'].values\n",
    "    Class = all_dataframe['Class'].values\n",
    "    kfold = KFold(n_splits=fold_number, shuffle=True)\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, valid_index in kfold.split(sequence_label, Class):\n",
    "        print(\"\\n========== Fold \" + str(fold + 1) + \" ==========\")\n",
    "        train_dataframe = all_dataframe.iloc[train_index, :]\n",
    "        valid_dataframe = all_dataframe.iloc[valid_index, :]\n",
    "        print(\"Training on\", str(train_dataframe.shape[0]), \"examples, Validation on\", str(valid_dataframe.shape[0]),\n",
    "              \"examples\")\n",
    "        model = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "\n",
    "        train(model, train_dataframe, valid_dataframe, fold + 1)\n",
    "        fold += 1\n",
    "\n",
    "train_dataframe = pd.read_csv(\"C:\\\\Users\\\\johnkwon\\\\Desktop\\\\PDB_seed_AF3\\\\binary\\\\Data\\\\GCN_binary_glo_glu_train.csv\")\n",
    "train_all(train_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
